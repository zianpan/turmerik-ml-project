{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data/example_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-6-6/68f994703e838fff13b9e54ab72d5ae44f184cb55300a7f65f46a7d282e8da23?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1731439452&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTQzOTQ1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tNi02LzY4Zjk5NDcwM2U4MzhmZmYxM2I5ZTU0YWI3MmQ1YWU0NGYxODRjYjU1MzAwYTdmNjVmNDZhN2QyODJlOGRhMjM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=MgriNufzU%7EVINKvHUaTmTwQDz80voHXdMtc7RY4aLs8eg0wWLztFiYIXrWVgyJ3LRDeX7FWjO9bVurKHAuFaqZl6Ukb3E9cEGA67T5H8UnO3DqlGya3gRwMU5eSSrpPRBfiVOSs9hvQ6m3Xv6pHm5-VxW5HzonayaVZoVvi-qc2DWyWtiQIIgZvea2L1Yois45Oon3TvbxJbB2ZuH6LCmwgpeFgfs7mosriMoHuksvrPm8l73bNE%7E1JlOKvaGpV6IgQgohAPC2ztvZI1esEIdVLDqwK8EPuZntqGU8uJCXL5CxybUTbyeKzrRa1v-lt8%7EJgWtLNnLefez0ilXhhA%7Eg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"You can get to CTM in different ways and yes, it's definitely possible to do so after some experience in start-up. However, I read that the most common route would be by being a CRA first instead. Something like CRA-Sr. CRA-CTM. \n",
    "If you start from study start-up you might have to spend some time as a regulatory or contract specialist before moving on as a Project Manager and then apply for CTM roles. Mind you though, this is what I see from people joining and leaving the company I work for.\n",
    "\n",
    "The only reference point I have regarding salaries, is for a CRA position back in 2022; 38k base + 5k car allowance. That was outside of London. You'll definitely find more in the pinned post in this sub.\n",
    "I was out of university and unemployed, was sending my CV to so many jobs both research and non-research, even applied for some mcjobs and spending hours tailoring my CV and spending lots of time making cover letters for those jobs. \n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Language models are increasingly being deployed for general problem solving\n",
    "across a wide range of tasks, but are still confined to token-level, left-to-right\n",
    "decision-making processes during inference. This means they can fall short in\n",
    "tasks that require exploration, strategic lookahead, or where initial decisions play\n",
    "a pivotal role. To surmount these challenges, we introduce a new framework for\n",
    "language model inference, “Tree of Thoughts” (ToT), which generalizes over the\n",
    "popular “Chain of Thought” approach to prompting language models, and enables\n",
    "exploration over coherent units of text (“thoughts”) that serve as intermediate steps\n",
    "toward problem solving. ToT allows LMs to perform deliberate decision making\n",
    "by considering multiple different reasoning paths and self-evaluating choices to\n",
    "decide the next course of action, as well as looking ahead or backtracking when\n",
    "necessary to make global choices. Our experiments show that ToT significantly\n",
    "enhances language models problem-solving abilities on three novel tasks requiring\n",
    "non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\n",
    "For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\n",
    "solved 4%of tasks, our method achieved a success rate of 74%. Code repo with all\n",
    "prompts: https://github.com/princeton-nlp/tree-of-thought-llm.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipe_sum(\"In 100 words, Can you summarize the following text\\n \" + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' In our series of letters from the University of New York.'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clinicalresearch+clinical trial', 'clinicalresearch+research study', 'clinicalresearch+new treatment', 'clinicalresearch+volunteer', 'health+clinical trial', 'health+research study', 'health+new treatment', 'health+volunteer', 'depression+clinical trial', 'depression+research study', 'depression+new treatment', 'depression+volunteer', 'mentalhealth+clinical trial', 'mentalhealth+research study', 'mentalhealth+new treatment', 'mentalhealth+volunteer', 'science+clinical trial', 'science+research study', 'science+new treatment', 'science+volunteer', 'cancer+clinical trial', 'cancer+research study', 'cancer+new treatment', 'cancer+volunteer'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_li = data['clinicalresearch+clinical trial']['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Your max_length is set to 142, but your input_length is only 140. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in trange(len(posts_li)):\n",
    "    # print(posts_li[i]['title'])\n",
    "    comments_li = posts_li[i]['comments']\n",
    "    for j in trange(len(comments_li)):\n",
    "        # print(comments_li[j]['body'])\n",
    "        content = comments_li[j]['content']\n",
    "        word_count = len(content.split())\n",
    "        if word_count > 80:\n",
    "            content2 = pipe_sum(\"In 50 words, Can you summarize the following text\\n \" + content)[0]['summary_text']\n",
    "            # print(content2)\n",
    "        else:\n",
    "            content2 = content\n",
    "            \n",
    "        sa = pipe(content2)[0]\n",
    "        # print(sa)\n",
    "        # {'label': 'POS', 'score': 0.9905712604522705}\n",
    "        label = sa['label']\n",
    "        score = sa['score']\n",
    "        res.append([content2, label, score])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = comments_li[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/sentiment/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POS', 'score': 0.9905712604522705}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"I like this movie\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
